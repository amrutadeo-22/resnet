{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrutadeo-22/resnet/blob/main/Eff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQcZS3KEVFia",
        "outputId": "b9a43e57-971c-4f34-8b74-24d46bd11a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Epoch 1/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 1s/step - accuracy: 0.2853 - loss: 1.9542 - val_accuracy: 0.4241 - val_loss: 2.1079\n",
            "Epoch 2/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1005s\u001b[0m 1s/step - accuracy: 0.4933 - loss: 1.4291 - val_accuracy: 0.3762 - val_loss: 6.2052\n",
            "Epoch 3/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1031s\u001b[0m 1s/step - accuracy: 0.5430 - loss: 1.2889 - val_accuracy: 0.5412 - val_loss: 1.3599\n",
            "Epoch 4/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1043s\u001b[0m 1s/step - accuracy: 0.5758 - loss: 1.2323 - val_accuracy: 0.5704 - val_loss: 1.2976\n",
            "Epoch 5/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1030s\u001b[0m 1s/step - accuracy: 0.6406 - loss: 1.0391 - val_accuracy: 0.6224 - val_loss: 1.1260\n",
            "Epoch 6/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1042s\u001b[0m 1s/step - accuracy: 0.6762 - loss: 0.9245 - val_accuracy: 0.6327 - val_loss: 1.1321\n",
            "Epoch 7/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1030s\u001b[0m 1s/step - accuracy: 0.6925 - loss: 0.8833 - val_accuracy: 0.6281 - val_loss: 1.2039\n",
            "Epoch 8/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m978s\u001b[0m 1s/step - accuracy: 0.7226 - loss: 0.8073 - val_accuracy: 0.7098 - val_loss: 0.8594\n",
            "Epoch 9/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1008s\u001b[0m 1s/step - accuracy: 0.7546 - loss: 0.7127 - val_accuracy: 0.6207 - val_loss: 1.2269\n",
            "Epoch 10/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1019s\u001b[0m 1s/step - accuracy: 0.7353 - loss: 0.7648 - val_accuracy: 0.7269 - val_loss: 0.8223\n",
            "Epoch 11/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 1s/step - accuracy: 0.7909 - loss: 0.6042 - val_accuracy: 0.7350 - val_loss: 0.8385\n",
            "Epoch 12/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m989s\u001b[0m 1s/step - accuracy: 0.8067 - loss: 0.5708 - val_accuracy: 0.7211 - val_loss: 0.8653\n",
            "Epoch 13/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1037s\u001b[0m 1s/step - accuracy: 0.8137 - loss: 0.5394 - val_accuracy: 0.7423 - val_loss: 0.7905\n",
            "Epoch 14/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m983s\u001b[0m 1s/step - accuracy: 0.8460 - loss: 0.4447 - val_accuracy: 0.7467 - val_loss: 0.8445\n",
            "Epoch 15/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m992s\u001b[0m 1s/step - accuracy: 0.8592 - loss: 0.4033 - val_accuracy: 0.7240 - val_loss: 0.9036\n",
            "Epoch 16/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1059s\u001b[0m 1s/step - accuracy: 0.8528 - loss: 0.4339 - val_accuracy: 0.7379 - val_loss: 0.8466\n",
            "Epoch 17/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1027s\u001b[0m 1s/step - accuracy: 0.8576 - loss: 0.4202 - val_accuracy: 0.7465 - val_loss: 0.8573\n",
            "Epoch 18/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1045s\u001b[0m 1s/step - accuracy: 0.8773 - loss: 0.3599 - val_accuracy: 0.7595 - val_loss: 0.8292\n",
            "Epoch 19/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1040s\u001b[0m 1s/step - accuracy: 0.9088 - loss: 0.2668 - val_accuracy: 0.7528 - val_loss: 0.9160\n",
            "Epoch 20/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1062s\u001b[0m 1s/step - accuracy: 0.9163 - loss: 0.2448 - val_accuracy: 0.7558 - val_loss: 0.9244\n",
            "Epoch 21/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1028s\u001b[0m 1s/step - accuracy: 0.9124 - loss: 0.2570 - val_accuracy: 0.7481 - val_loss: 0.9379\n",
            "Epoch 22/25\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m992s\u001b[0m 1s/step - accuracy: 0.8926 - loss: 0.3241 - val_accuracy: 0.7477 - val_loss: 0.8708\n",
            "Epoch 23/25\n",
            "\u001b[1m675/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2:12\u001b[0m 1s/step - accuracy: 0.9265 - loss: 0.2115"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets, utils\n",
        "\n",
        "# Swish activation function\n",
        "def swish(x):\n",
        "    return x * tf.nn.sigmoid(x)\n",
        "\n",
        "# Squeeze and Excitation block\n",
        "def squeeze_and_excite(inputs, reduction=4):\n",
        "    filters = inputs.shape[-1]\n",
        "    se = layers.GlobalAveragePooling2D()(inputs)\n",
        "    se = layers.Reshape((1, 1, filters))(se)\n",
        "    se = layers.Dense(filters // reduction, activation='relu')(se)\n",
        "    se = layers.Dense(filters, activation='sigmoid')(se)\n",
        "    return layers.Multiply()([inputs, se])\n",
        "\n",
        "# MBConv block\n",
        "def mb_conv_block(inputs, filters, expansion_factor, kernel_size, strides, reduction=4):\n",
        "    x = layers.Conv2D(filters * expansion_factor, kernel_size=1, padding='same', use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(swish)(x)\n",
        "\n",
        "    x = layers.DepthwiseConv2D(kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(swish)(x)\n",
        "\n",
        "    x = squeeze_and_excite(x, reduction)\n",
        "\n",
        "    x = layers.Conv2D(filters, kernel_size=1, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if strides == 1 and inputs.shape[-1] == filters:\n",
        "        x = layers.Add()([x, inputs])\n",
        "\n",
        "    return x\n",
        "\n",
        "# EfficientNet model\n",
        "def efficient_net(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(swish)(x)\n",
        "\n",
        "    x = mb_conv_block(x, filters=16, expansion_factor=1, kernel_size=3, strides=1)\n",
        "\n",
        "    x = mb_conv_block(x, filters=24, expansion_factor=6, kernel_size=3, strides=2)\n",
        "    x = mb_conv_block(x, filters=24, expansion_factor=6, kernel_size=3, strides=1)\n",
        "\n",
        "    x = mb_conv_block(x, filters=40, expansion_factor=6, kernel_size=5, strides=2)\n",
        "    x = mb_conv_block(x, filters=40, expansion_factor=6, kernel_size=5, strides=1)\n",
        "\n",
        "    x = mb_conv_block(x, filters=80, expansion_factor=6, kernel_size=3, strides=2)\n",
        "    x = mb_conv_block(x, filters=80, expansion_factor=6, kernel_size=3, strides=1)\n",
        "\n",
        "    x = mb_conv_block(x, filters=112, expansion_factor=6, kernel_size=5, strides=1)\n",
        "    x = mb_conv_block(x, filters=112, expansion_factor=6, kernel_size=5, strides=1)\n",
        "\n",
        "    x = mb_conv_block(x, filters=192, expansion_factor=6, kernel_size=5, strides=2)\n",
        "    x = mb_conv_block(x, filters=192, expansion_factor=6, kernel_size=5, strides=1)\n",
        "\n",
        "    x = mb_conv_block(x, filters=320, expansion_factor=6, kernel_size=3, strides=1)\n",
        "\n",
        "    x = layers.Conv2D(1280, kernel_size=1, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(swish)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(input_train, target_train), (input_test, target_test) = datasets.cifar10.load_data()\n",
        "\n",
        "input_train = input_train.astype('float32') / 255.0\n",
        "input_test = input_test.astype('float32') / 255.0\n",
        "\n",
        "target_train = utils.to_categorical(target_train, 10)\n",
        "target_test = utils.to_categorical(target_test, 10)\n",
        "\n",
        "# Create model\n",
        "model = efficient_net(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(input_train, target_train, epochs=25, batch_size=64, validation_data=(input_test, target_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO33bVt6F5dFnfe65hfsc7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}