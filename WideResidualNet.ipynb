{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrutadeo-22/resnet/blob/main/WideResidualNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwUsDMIpbZm6",
        "outputId": "8a6ab821-f5b7-4860-d812-9973de1c175a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nested-dict\n",
            "  Downloading nested_dict-1.61.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchnet) (1.16.0)\n",
            "Collecting visdom (from torchnet)\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (6.3.3)\n",
            "Collecting jsonpatch (from visdom->torchnet)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.8.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom->torchnet)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2024.6.2)\n",
            "Building wheels for collected packages: torchnet, nested-dict, visdom\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29727 sha256=c87819bc2f73456ecee7f4e44fa60e48aea5ffe875b4ccf373f3a471446659d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/ae/94/9f5edd6871983f30967ad11d60ef434c3d1b007654de4c8065\n",
            "  Building wheel for nested-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nested-dict: filename=nested_dict-1.61-py3-none-any.whl size=6428 sha256=ca315df9629ca5b5c980b035b2c5b8580e3fd1251143531c62036a6ef17d3f80\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3a/52/1086c0367bbfdf3d8bc886930e1e6680a9e6a1d246d17f6b65\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=319f47d346f41be11ab9ca643933dce2ed5bc20c6d27bc1ec26aa68523c69d3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built torchnet nested-dict visdom\n",
            "Installing collected packages: nested-dict, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jsonpointer, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jsonpatch, visdom, nvidia-cusolver-cu12, torchnet\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 nested-dict-1.61 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchnet-0.0.4 visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torchvision torchnet nested-dict tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF1UzdzldRZe",
        "outputId": "e95f724b-b076-4de1-911f-7d9dfdc356d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing resnet.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile resnet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import utils\n",
        "\n",
        "\n",
        "def resnet(depth, width, num_classes):\n",
        "    assert (depth - 4) % 6 == 0, 'depth should be 6n+4'\n",
        "    n = (depth - 4) // 6\n",
        "    widths = [int(v * width) for v in (16, 32, 64)]\n",
        "\n",
        "    def gen_block_params(ni, no):\n",
        "        return {\n",
        "            'conv0': utils.conv_params(ni, no, 3),\n",
        "            'conv1': utils.conv_params(no, no, 3),\n",
        "            'bn0': utils.bnparams(ni),\n",
        "            'bn1': utils.bnparams(no),\n",
        "            'convdim': utils.conv_params(ni, no, 1) if ni != no else None,\n",
        "        }\n",
        "\n",
        "    def gen_group_params(ni, no, count):\n",
        "        return {'block%d' % i: gen_block_params(ni if i == 0 else no, no)\n",
        "                for i in range(count)}\n",
        "\n",
        "    flat_params = utils.cast(utils.flatten({\n",
        "        'conv0': utils.conv_params(3, 16, 3),\n",
        "        'group0': gen_group_params(16, widths[0], n),\n",
        "        'group1': gen_group_params(widths[0], widths[1], n),\n",
        "        'group2': gen_group_params(widths[1], widths[2], n),\n",
        "        'bn': utils.bnparams(widths[2]),\n",
        "        'fc': utils.linear_params(widths[2], num_classes),\n",
        "    }))\n",
        "\n",
        "    utils.set_requires_grad_except_bn_(flat_params)\n",
        "\n",
        "    def block(x, params, base, mode, stride):\n",
        "        o1 = F.relu(utils.batch_norm(x, params, base + '.bn0', mode), inplace=True)\n",
        "        y = F.conv2d(o1, params[base + '.conv0'], stride=stride, padding=1)\n",
        "        o2 = F.relu(utils.batch_norm(y, params, base + '.bn1', mode), inplace=True)\n",
        "        z = F.conv2d(o2, params[base + '.conv1'], stride=1, padding=1)\n",
        "        if base + '.convdim' in params:\n",
        "            return z + F.conv2d(o1, params[base + '.convdim'], stride=stride)\n",
        "        else:\n",
        "            return z + x\n",
        "\n",
        "    def group(o, params, base, mode, stride):\n",
        "        for i in range(n):\n",
        "            o = block(o, params, '%s.block%d' % (base,i), mode, stride if i == 0 else 1)\n",
        "        return o\n",
        "\n",
        "    def f(input, params, mode):\n",
        "        x = F.conv2d(input, params['conv0'], padding=1)\n",
        "        g0 = group(x, params, 'group0', mode, 1)\n",
        "        g1 = group(g0, params, 'group1', mode, 2)\n",
        "        g2 = group(g1, params, 'group2', mode, 2)\n",
        "        o = F.relu(utils.batch_norm(g2, params, 'bn', mode))\n",
        "        o = F.avg_pool2d(o, 8, 1, 0)\n",
        "        o = o.view(o.size(0), -1)\n",
        "        o = F.linear(o, params['fc.weight'], params['fc.bias'])\n",
        "        return o\n",
        "\n",
        "    return f, flat_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBq_oF4Af9os",
        "outputId": "ddd6b52c-77d5-4342-f70d-845c696f817b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import torch\n",
        "from torch.nn.init import kaiming_normal_\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel._functions import Broadcast\n",
        "from torch.nn.parallel import scatter, parallel_apply, gather\n",
        "from functools import partial\n",
        "from nested_dict import nested_dict\n",
        "\n",
        "\n",
        "def cast(params, dtype='float'):\n",
        "    if isinstance(params, dict):\n",
        "        return {k: cast(v, dtype) for k,v in params.items()}\n",
        "    else:\n",
        "        return getattr(params.cuda() if torch.cuda.is_available() else params, dtype)()\n",
        "\n",
        "\n",
        "def conv_params(ni, no, k=1):\n",
        "    return kaiming_normal_(torch.Tensor(no, ni, k, k))\n",
        "\n",
        "\n",
        "def linear_params(ni, no):\n",
        "    return {'weight': kaiming_normal_(torch.Tensor(no, ni)), 'bias': torch.zeros(no)}\n",
        "\n",
        "\n",
        "def bnparams(n):\n",
        "    return {'weight': torch.rand(n),\n",
        "            'bias': torch.zeros(n),\n",
        "            'running_mean': torch.zeros(n),\n",
        "            'running_var': torch.ones(n)}\n",
        "\n",
        "\n",
        "def data_parallel(f, input, params, mode, device_ids, output_device=None):\n",
        "    assert isinstance(device_ids, list)\n",
        "    if output_device is None:\n",
        "        output_device = device_ids[0]\n",
        "\n",
        "    if len(device_ids) == 1:\n",
        "        return f(input, params, mode)\n",
        "\n",
        "    params_all = Broadcast.apply(device_ids, *params.values())\n",
        "    params_replicas = [{k: params_all[i + j*len(params)] for i, k in enumerate(params.keys())}\n",
        "                       for j in range(len(device_ids))]\n",
        "\n",
        "    replicas = [partial(f, params=p, mode=mode)\n",
        "                for p in params_replicas]\n",
        "    inputs = scatter([input], device_ids)\n",
        "    outputs = parallel_apply(replicas, inputs)\n",
        "    return gather(outputs, output_device)\n",
        "\n",
        "\n",
        "def flatten(params):\n",
        "    return {'.'.join(k): v for k, v in nested_dict(params).items_flat() if v is not None}\n",
        "\n",
        "\n",
        "def batch_norm(x, params, base, mode):\n",
        "    return F.batch_norm(x, weight=params[base + '.weight'],\n",
        "                        bias=params[base + '.bias'],\n",
        "                        running_mean=params[base + '.running_mean'],\n",
        "                        running_var=params[base + '.running_var'],\n",
        "                        training=mode)\n",
        "\n",
        "\n",
        "def print_tensor_dict(params):\n",
        "    kmax = max(len(key) for key in params.keys())\n",
        "    for i, (key, v) in enumerate(params.items()):\n",
        "        print(str(i).ljust(5), key.ljust(kmax + 3), str(tuple(v.shape)).ljust(23), torch.typename(v), v.requires_grad)\n",
        "\n",
        "\n",
        "def set_requires_grad_except_bn_(params):\n",
        "    for k, v in params.items():\n",
        "        if not k.endswith('running_mean') and not k.endswith('running_var'):\n",
        "            v.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScbZVs82gB6m",
        "outputId": "ad990bea-1679-4e2f-d509-8257ebfe21d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "\n",
        "\"\"\"\n",
        "    PyTorch training code for Wide Residual Networks:\n",
        "    http://arxiv.org/abs/1605.07146\n",
        "\n",
        "    The code reproduces *exactly* it's lua version:\n",
        "    https://github.com/szagoruyko/wide-residual-networks\n",
        "\n",
        "    2016 Sergey Zagoruyko\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchnet as tnt\n",
        "from torchnet.engine import Engine\n",
        "from utils import cast, data_parallel, print_tensor_dict\n",
        "from torch.backends import cudnn\n",
        "from resnet import resnet\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Wide Residual Networks')\n",
        "# Model options\n",
        "parser.add_argument('--model', default='resnet', type=str)\n",
        "parser.add_argument('--depth', default=16, type=int)\n",
        "parser.add_argument('--width', default=1, type=float)\n",
        "parser.add_argument('--dataset', default='CIFAR10', type=str)\n",
        "parser.add_argument('--dataroot', default='.', type=str)\n",
        "parser.add_argument('--dtype', default='float', type=str)\n",
        "parser.add_argument('--groups', default=1, type=int)\n",
        "parser.add_argument('--nthread', default=4, type=int)\n",
        "parser.add_argument('--seed', default=1, type=int)\n",
        "\n",
        "# Training options\n",
        "parser.add_argument('--batch_size', default=128, type=int)\n",
        "parser.add_argument('--lr', default=0.1, type=float)\n",
        "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--weight_decay', default=0.0005, type=float)\n",
        "parser.add_argument('--epoch_step', default='[60,120,160]', type=str,\n",
        "                    help='json list with epochs to drop lr on')\n",
        "parser.add_argument('--lr_decay_ratio', default=0.2, type=float)\n",
        "parser.add_argument('--resume', default='', type=str)\n",
        "parser.add_argument('--note', default='', type=str)\n",
        "\n",
        "# Device options\n",
        "parser.add_argument('--cuda', action='store_true')\n",
        "parser.add_argument('--save', default='', type=str,\n",
        "                    help='save parameters and logs in this folder')\n",
        "parser.add_argument('--ngpu', default=1, type=int,\n",
        "                    help='number of GPUs to use for training')\n",
        "parser.add_argument('--gpu_id', default='0', type=str,\n",
        "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
        "\n",
        "\n",
        "def create_dataset(opt, train):\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(np.array([125.3, 123.0, 113.9]) / 255.0,\n",
        "                    np.array([63.0, 62.1, 66.7]) / 255.0),\n",
        "    ])\n",
        "    if train:\n",
        "        transform = T.Compose([\n",
        "            T.Pad(4, padding_mode='reflect'),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomCrop(32),\n",
        "            transform\n",
        "        ])\n",
        "    return getattr(datasets, opt.dataset)(opt.dataroot, train=train, download=True, transform=transform)\n",
        "\n",
        "\n",
        "def main():\n",
        "    opt = parser.parse_args()\n",
        "    print('parsed options:', vars(opt))\n",
        "    epoch_step = json.loads(opt.epoch_step)\n",
        "    num_classes = 10 if opt.dataset == 'CIFAR10' else 100\n",
        "\n",
        "    torch.manual_seed(opt.seed)\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu_id\n",
        "\n",
        "    def create_iterator(mode):\n",
        "        return DataLoader(create_dataset(opt, mode), opt.batch_size, shuffle=mode,\n",
        "                          num_workers=opt.nthread, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    train_loader = create_iterator(True)\n",
        "    test_loader = create_iterator(False)\n",
        "\n",
        "    f, params = resnet(opt.depth, opt.width, num_classes)\n",
        "\n",
        "    def create_optimizer(opt, lr):\n",
        "        print('creating optimizer with lr = ', lr)\n",
        "        return SGD([v for v in params.values() if v.requires_grad], lr, momentum=0.9, weight_decay=opt.weight_decay)\n",
        "\n",
        "    optimizer = create_optimizer(opt, opt.lr)\n",
        "\n",
        "    epoch = 0\n",
        "    if opt.resume != '':\n",
        "        state_dict = torch.load(opt.resume)\n",
        "        epoch = state_dict['epoch']\n",
        "        params_tensors = state_dict['params']\n",
        "        for k, v in params.items():\n",
        "            v.data.copy_(params_tensors[k])\n",
        "        optimizer.load_state_dict(state_dict['optimizer'])\n",
        "\n",
        "    print('\\nParameters:')\n",
        "    print_tensor_dict(params)\n",
        "\n",
        "    n_parameters = sum(p.numel() for p in params.values() if p.requires_grad)\n",
        "    print('\\nTotal number of parameters:', n_parameters)\n",
        "\n",
        "    meter_loss = tnt.meter.AverageValueMeter()\n",
        "    classacc = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    timer_train = tnt.meter.TimeMeter('s')\n",
        "    timer_test = tnt.meter.TimeMeter('s')\n",
        "\n",
        "    if not os.path.exists(opt.save):\n",
        "        os.mkdir(opt.save)\n",
        "\n",
        "    def h(sample):\n",
        "        inputs = cast(sample[0], opt.dtype)\n",
        "        targets = cast(sample[1], 'long')\n",
        "        y = data_parallel(f, inputs, params, sample[2], list(range(opt.ngpu))).float()\n",
        "        return F.cross_entropy(y, targets), y\n",
        "\n",
        "    def log(t, state):\n",
        "        torch.save(dict(params=params, epoch=t['epoch'], optimizer=state['optimizer'].state_dict()),\n",
        "                   os.path.join(opt.save, 'model.pt7'))\n",
        "        z = {**vars(opt), **t}\n",
        "        with open(os.path.join(opt.save, 'log.txt'), 'a') as flog:\n",
        "            flog.write('json_stats: ' + json.dumps(z) + '\\n')\n",
        "        print(z)\n",
        "\n",
        "    def on_sample(state):\n",
        "        state['sample'].append(state['train'])\n",
        "\n",
        "    def on_forward(state):\n",
        "        loss = float(state['loss'])\n",
        "        classacc.add(state['output'].data, state['sample'][1])\n",
        "        meter_loss.add(loss)\n",
        "        if state['train']:\n",
        "            state['iterator'].set_postfix(loss=loss)\n",
        "\n",
        "    def on_start(state):\n",
        "        state['epoch'] = epoch\n",
        "\n",
        "    def on_start_epoch(state):\n",
        "        classacc.reset()\n",
        "        meter_loss.reset()\n",
        "        timer_train.reset()\n",
        "        state['iterator'] = tqdm(train_loader, dynamic_ncols=True)\n",
        "\n",
        "        epoch = state['epoch'] + 1\n",
        "        if epoch in epoch_step:\n",
        "            lr = state['optimizer'].param_groups[0]['lr']\n",
        "            state['optimizer'] = create_optimizer(opt, lr * opt.lr_decay_ratio)\n",
        "\n",
        "    def on_end_epoch(state):\n",
        "        train_loss = meter_loss.value()\n",
        "        train_acc = classacc.value()\n",
        "        train_time = timer_train.value()\n",
        "        meter_loss.reset()\n",
        "        classacc.reset()\n",
        "        timer_test.reset()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            engine.test(h, test_loader)\n",
        "\n",
        "        test_acc = classacc.value()[0]\n",
        "        print(log({\n",
        "            \"train_loss\": train_loss[0],\n",
        "            \"train_acc\": train_acc[0],\n",
        "            \"test_loss\": meter_loss.value()[0],\n",
        "            \"test_acc\": test_acc,\n",
        "            \"epoch\": state['epoch'],\n",
        "            \"num_classes\": num_classes,\n",
        "            \"n_parameters\": n_parameters,\n",
        "            \"train_time\": train_time,\n",
        "            \"test_time\": timer_test.value(),\n",
        "        }, state))\n",
        "        print('==> id: %s (%d/%d), test_acc: \\33[91m%.2f\\033[0m' %\n",
        "              (opt.save, state['epoch'], opt.epochs, test_acc))\n",
        "\n",
        "    engine = Engine()\n",
        "    engine.hooks['on_sample'] = on_sample\n",
        "    engine.hooks['on_forward'] = on_forward\n",
        "    engine.hooks['on_start_epoch'] = on_start_epoch\n",
        "    engine.hooks['on_end_epoch'] = on_end_epoch\n",
        "    engine.hooks['on_start'] = on_start\n",
        "    engine.train(h, train_loader, opt.epochs, optimizer)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gur_GYbXgEwG",
        "outputId": "39f76975-53be-44a1-9619-033f55aca397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parsed options: {'model': 'resnet', 'depth': 28, 'width': 10.0, 'dataset': 'CIFAR10', 'dataroot': './data', 'dtype': 'float', 'groups': 1, 'nthread': 4, 'seed': 1, 'batch_size': 128, 'lr': 0.1, 'epochs': 200, 'weight_decay': 0.0005, 'epoch_step': '[60,120,160]', 'lr_decay_ratio': 0.2, 'resume': '', 'note': '', 'cuda': True, 'save': './save', 'ngpu': 1, 'gpu_id': '0'}\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:03<00:00, 56698501.89it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Files already downloaded and verified\n",
            "creating optimizer with lr =  0.1\n",
            "\n",
            "Parameters:\n",
            "0     conv0                             (16, 3, 3, 3)           torch.FloatTensor True\n",
            "1     group0.block0.conv0               (160, 16, 3, 3)         torch.FloatTensor True\n",
            "2     group0.block0.conv1               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "3     group0.block0.bn0.weight          (16,)                   torch.FloatTensor True\n",
            "4     group0.block0.bn0.bias            (16,)                   torch.FloatTensor True\n",
            "5     group0.block0.bn0.running_mean    (16,)                   torch.FloatTensor False\n",
            "6     group0.block0.bn0.running_var     (16,)                   torch.FloatTensor False\n",
            "7     group0.block0.bn1.weight          (160,)                  torch.FloatTensor True\n",
            "8     group0.block0.bn1.bias            (160,)                  torch.FloatTensor True\n",
            "9     group0.block0.bn1.running_mean    (160,)                  torch.FloatTensor False\n",
            "10    group0.block0.bn1.running_var     (160,)                  torch.FloatTensor False\n",
            "11    group0.block0.convdim             (160, 16, 1, 1)         torch.FloatTensor True\n",
            "12    group0.block1.conv0               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "13    group0.block1.conv1               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "14    group0.block1.bn0.weight          (160,)                  torch.FloatTensor True\n",
            "15    group0.block1.bn0.bias            (160,)                  torch.FloatTensor True\n",
            "16    group0.block1.bn0.running_mean    (160,)                  torch.FloatTensor False\n",
            "17    group0.block1.bn0.running_var     (160,)                  torch.FloatTensor False\n",
            "18    group0.block1.bn1.weight          (160,)                  torch.FloatTensor True\n",
            "19    group0.block1.bn1.bias            (160,)                  torch.FloatTensor True\n",
            "20    group0.block1.bn1.running_mean    (160,)                  torch.FloatTensor False\n",
            "21    group0.block1.bn1.running_var     (160,)                  torch.FloatTensor False\n",
            "22    group0.block2.conv0               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "23    group0.block2.conv1               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "24    group0.block2.bn0.weight          (160,)                  torch.FloatTensor True\n",
            "25    group0.block2.bn0.bias            (160,)                  torch.FloatTensor True\n",
            "26    group0.block2.bn0.running_mean    (160,)                  torch.FloatTensor False\n",
            "27    group0.block2.bn0.running_var     (160,)                  torch.FloatTensor False\n",
            "28    group0.block2.bn1.weight          (160,)                  torch.FloatTensor True\n",
            "29    group0.block2.bn1.bias            (160,)                  torch.FloatTensor True\n",
            "30    group0.block2.bn1.running_mean    (160,)                  torch.FloatTensor False\n",
            "31    group0.block2.bn1.running_var     (160,)                  torch.FloatTensor False\n",
            "32    group0.block3.conv0               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "33    group0.block3.conv1               (160, 160, 3, 3)        torch.FloatTensor True\n",
            "34    group0.block3.bn0.weight          (160,)                  torch.FloatTensor True\n",
            "35    group0.block3.bn0.bias            (160,)                  torch.FloatTensor True\n",
            "36    group0.block3.bn0.running_mean    (160,)                  torch.FloatTensor False\n",
            "37    group0.block3.bn0.running_var     (160,)                  torch.FloatTensor False\n",
            "38    group0.block3.bn1.weight          (160,)                  torch.FloatTensor True\n",
            "39    group0.block3.bn1.bias            (160,)                  torch.FloatTensor True\n",
            "40    group0.block3.bn1.running_mean    (160,)                  torch.FloatTensor False\n",
            "41    group0.block3.bn1.running_var     (160,)                  torch.FloatTensor False\n",
            "42    group1.block0.conv0               (320, 160, 3, 3)        torch.FloatTensor True\n",
            "43    group1.block0.conv1               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "44    group1.block0.bn0.weight          (160,)                  torch.FloatTensor True\n",
            "45    group1.block0.bn0.bias            (160,)                  torch.FloatTensor True\n",
            "46    group1.block0.bn0.running_mean    (160,)                  torch.FloatTensor False\n",
            "47    group1.block0.bn0.running_var     (160,)                  torch.FloatTensor False\n",
            "48    group1.block0.bn1.weight          (320,)                  torch.FloatTensor True\n",
            "49    group1.block0.bn1.bias            (320,)                  torch.FloatTensor True\n",
            "50    group1.block0.bn1.running_mean    (320,)                  torch.FloatTensor False\n",
            "51    group1.block0.bn1.running_var     (320,)                  torch.FloatTensor False\n",
            "52    group1.block0.convdim             (320, 160, 1, 1)        torch.FloatTensor True\n",
            "53    group1.block1.conv0               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "54    group1.block1.conv1               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "55    group1.block1.bn0.weight          (320,)                  torch.FloatTensor True\n",
            "56    group1.block1.bn0.bias            (320,)                  torch.FloatTensor True\n",
            "57    group1.block1.bn0.running_mean    (320,)                  torch.FloatTensor False\n",
            "58    group1.block1.bn0.running_var     (320,)                  torch.FloatTensor False\n",
            "59    group1.block1.bn1.weight          (320,)                  torch.FloatTensor True\n",
            "60    group1.block1.bn1.bias            (320,)                  torch.FloatTensor True\n",
            "61    group1.block1.bn1.running_mean    (320,)                  torch.FloatTensor False\n",
            "62    group1.block1.bn1.running_var     (320,)                  torch.FloatTensor False\n",
            "63    group1.block2.conv0               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "64    group1.block2.conv1               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "65    group1.block2.bn0.weight          (320,)                  torch.FloatTensor True\n",
            "66    group1.block2.bn0.bias            (320,)                  torch.FloatTensor True\n",
            "67    group1.block2.bn0.running_mean    (320,)                  torch.FloatTensor False\n",
            "68    group1.block2.bn0.running_var     (320,)                  torch.FloatTensor False\n",
            "69    group1.block2.bn1.weight          (320,)                  torch.FloatTensor True\n",
            "70    group1.block2.bn1.bias            (320,)                  torch.FloatTensor True\n",
            "71    group1.block2.bn1.running_mean    (320,)                  torch.FloatTensor False\n",
            "72    group1.block2.bn1.running_var     (320,)                  torch.FloatTensor False\n",
            "73    group1.block3.conv0               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "74    group1.block3.conv1               (320, 320, 3, 3)        torch.FloatTensor True\n",
            "75    group1.block3.bn0.weight          (320,)                  torch.FloatTensor True\n",
            "76    group1.block3.bn0.bias            (320,)                  torch.FloatTensor True\n",
            "77    group1.block3.bn0.running_mean    (320,)                  torch.FloatTensor False\n",
            "78    group1.block3.bn0.running_var     (320,)                  torch.FloatTensor False\n",
            "79    group1.block3.bn1.weight          (320,)                  torch.FloatTensor True\n",
            "80    group1.block3.bn1.bias            (320,)                  torch.FloatTensor True\n",
            "81    group1.block3.bn1.running_mean    (320,)                  torch.FloatTensor False\n",
            "82    group1.block3.bn1.running_var     (320,)                  torch.FloatTensor False\n",
            "83    group2.block0.conv0               (640, 320, 3, 3)        torch.FloatTensor True\n",
            "84    group2.block0.conv1               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "85    group2.block0.bn0.weight          (320,)                  torch.FloatTensor True\n",
            "86    group2.block0.bn0.bias            (320,)                  torch.FloatTensor True\n",
            "87    group2.block0.bn0.running_mean    (320,)                  torch.FloatTensor False\n",
            "88    group2.block0.bn0.running_var     (320,)                  torch.FloatTensor False\n",
            "89    group2.block0.bn1.weight          (640,)                  torch.FloatTensor True\n",
            "90    group2.block0.bn1.bias            (640,)                  torch.FloatTensor True\n",
            "91    group2.block0.bn1.running_mean    (640,)                  torch.FloatTensor False\n",
            "92    group2.block0.bn1.running_var     (640,)                  torch.FloatTensor False\n",
            "93    group2.block0.convdim             (640, 320, 1, 1)        torch.FloatTensor True\n",
            "94    group2.block1.conv0               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "95    group2.block1.conv1               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "96    group2.block1.bn0.weight          (640,)                  torch.FloatTensor True\n",
            "97    group2.block1.bn0.bias            (640,)                  torch.FloatTensor True\n",
            "98    group2.block1.bn0.running_mean    (640,)                  torch.FloatTensor False\n",
            "99    group2.block1.bn0.running_var     (640,)                  torch.FloatTensor False\n",
            "100   group2.block1.bn1.weight          (640,)                  torch.FloatTensor True\n",
            "101   group2.block1.bn1.bias            (640,)                  torch.FloatTensor True\n",
            "102   group2.block1.bn1.running_mean    (640,)                  torch.FloatTensor False\n",
            "103   group2.block1.bn1.running_var     (640,)                  torch.FloatTensor False\n",
            "104   group2.block2.conv0               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "105   group2.block2.conv1               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "106   group2.block2.bn0.weight          (640,)                  torch.FloatTensor True\n",
            "107   group2.block2.bn0.bias            (640,)                  torch.FloatTensor True\n",
            "108   group2.block2.bn0.running_mean    (640,)                  torch.FloatTensor False\n",
            "109   group2.block2.bn0.running_var     (640,)                  torch.FloatTensor False\n",
            "110   group2.block2.bn1.weight          (640,)                  torch.FloatTensor True\n",
            "111   group2.block2.bn1.bias            (640,)                  torch.FloatTensor True\n",
            "112   group2.block2.bn1.running_mean    (640,)                  torch.FloatTensor False\n",
            "113   group2.block2.bn1.running_var     (640,)                  torch.FloatTensor False\n",
            "114   group2.block3.conv0               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "115   group2.block3.conv1               (640, 640, 3, 3)        torch.FloatTensor True\n",
            "116   group2.block3.bn0.weight          (640,)                  torch.FloatTensor True\n",
            "117   group2.block3.bn0.bias            (640,)                  torch.FloatTensor True\n",
            "118   group2.block3.bn0.running_mean    (640,)                  torch.FloatTensor False\n",
            "119   group2.block3.bn0.running_var     (640,)                  torch.FloatTensor False\n",
            "120   group2.block3.bn1.weight          (640,)                  torch.FloatTensor True\n",
            "121   group2.block3.bn1.bias            (640,)                  torch.FloatTensor True\n",
            "122   group2.block3.bn1.running_mean    (640,)                  torch.FloatTensor False\n",
            "123   group2.block3.bn1.running_var     (640,)                  torch.FloatTensor False\n",
            "124   bn.weight                         (640,)                  torch.FloatTensor True\n",
            "125   bn.bias                           (640,)                  torch.FloatTensor True\n",
            "126   bn.running_mean                   (640,)                  torch.FloatTensor False\n",
            "127   bn.running_var                    (640,)                  torch.FloatTensor False\n",
            "128   fc.weight                         (10, 640)               torch.FloatTensor True\n",
            "129   fc.bias                           (10,)                   torch.FloatTensor True\n",
            "\n",
            "Total number of parameters: 36479194\n",
            " 72% 283/391 [5:49:01<2:11:38, 73.14s/it, loss=1.37]"
          ]
        }
      ],
      "source": [
        "!python main.py --cuda --ngpu 1 --depth 28 --width 10 --batch_size 128 --dataset CIFAR10 --dataroot ./data --save ./save\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6QRatBgXy4qhEPSIpz6z8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}